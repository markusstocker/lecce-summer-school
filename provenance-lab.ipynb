{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Provenance with Python and Semantic Technologies\n",
    "\n",
    "In this lab class, we look at a scientific workflow for interpreting aerosol observational data as measured by an observation system that is part of a larger [research infrastructure](https://ec.europa.eu/research/infrastructures/?pg=about) in order to detect the occurrence of [new particle formation events](https://www.ebi.ac.uk/ols/ontologies/envo/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FENVO_01001359) on selected days in [Hyyti채l채](http://sws.geonames.org/656888/), Finland. Detected events are then described, specifically their duration. Finally, event descriptions are used to create a dataset from which we compute average event duration.\n",
    "\n",
    "While executing the workflow, we will generate numerous data files as well as [provenance](https://www.w3.org/TR/prov-o/) information. Provenance is recorded as [RDF](https://www.w3.org/RDF/) in files and we will look at them in detail as they are created. Finally, we will inspect the generated provenance using [SPARQL](https://www.w3.org/TR/sparql11-query/) queries.\n",
    "\n",
    "Before we start, we need to load required Python modules as well as a few functions used in the workflow. Let's load first the required Python modules. Please execute the following (and all other) code blocks using ALT+ENTER or the `Run` button in the menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import requests, io, pydot, json, glob, shutil, pathlib, pandas as pd, numpy as np\n",
    "from rdflib import Graph, URIRef, Literal, BNode\n",
    "from rdflib.namespace import RDF, XSD\n",
    "from rdflib.plugins.sparql.results.csvresults import CSVResultSerializer\n",
    "from matplotlib import pyplot as plt\n",
    "from urllib.parse import urlencode\n",
    "from datetime import datetime, timedelta\n",
    "from pytz import timezone\n",
    "from rdflib import Graph, BNode\n",
    "from rdflib.namespace import RDF\n",
    "from rdflib.tools.rdf2dot import rdf2dot\n",
    "from IPython.display import display, Image, Markdown\n",
    "from prov.model import ProvDocument\n",
    "from prov.dot import prov_to_dot\n",
    "from shortid import ShortId\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The following code block creates a few sub folders needed to store data created in the workflow. If the folders exist we delete the content. Thus, if you like to start from scratch you can do so by executing the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "shutil.rmtree('average-duration', ignore_errors=True)\n",
    "shutil.rmtree('event-dataset', ignore_errors=True)\n",
    "shutil.rmtree('event-description', ignore_errors=True)\n",
    "shutil.rmtree('observational-data', ignore_errors=True)\n",
    "shutil.rmtree('provenance', ignore_errors=True)\n",
    "pathlib.Path('average-duration').mkdir()\n",
    "pathlib.Path('event-dataset').mkdir()\n",
    "pathlib.Path('event-description').mkdir()\n",
    "pathlib.Path('observational-data').mkdir()\n",
    "pathlib.Path('provenance').mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Next we load a few functions used in the workflow. \n",
    "\n",
    "The first is called `fetch_data` and implements the functionality needed to fetch aerosol observational data for a given date (e.g., `2013-04-04`) as measured by an observation system located in [Hyyti채l채](http://sws.geonames.org/656888/), Finland. The observation system is part of the Finnish [Station for Measuring Ecosystem-Atmosphere Relations](https://www.atm.helsinki.fi/SMEAR/) ([SMEAR](https://twitter.com/GlobalSMEAR)), an advanced research infrastructure with a history that goes back to the early '90. Specifically, the observation system is part of [SMEAR II](https://www.atm.helsinki.fi/SMEAR/index.php/smear-ii). Take a look at these links to get acquainted with SMEAR.\n",
    "\n",
    "Naturally, the observational data are not fetched directly from the observation system. Rather, they are processed and published by the research infrastructure. Data publishing is supported by [SmartSMEAR](https://avaa.tdata.fi/web/avaa/-/smartsmear), a data visualization and download tool (see also [Junninen et al.](https://helda.helsinki.fi/bitstream/handle/10138/233466/ber14-4-447.pdf)). Conveniently, SmartSMEAR provides an [API](https://avaa.tdata.fi/web/smart/smear/api) through which we can programmatically access the required data. This is the task of the `fetch_data` function defined next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def fetch_data(date):\n",
    "    time_from = timezone('Europe/Helsinki').localize(datetime.strptime(date, '%Y-%m-%d'))\n",
    "    time_to = time_from + timedelta(days=1)\n",
    "\n",
    "    query = {\n",
    "        'table': 'HYY_DMPS', 'quality': 'ANY', 'averaging': 'NONE', 'type': 'NONE',\n",
    "        'from': str(time_from), 'to': str(time_to), 'variables': 'd316e1,d355e1,d398e1,'\\\n",
    "        'd447e1,d501e1,d562e1,d631e1,d708e1,d794e1,d891e1,d100e2,d112e2,d126e2,d141e2,d158e2,'\\\n",
    "        'd178e2,d200e2,d224e2,d251e2,d282e2,d316e2,d355e2,d398e2,d447e2,d501e2,d562e2,d631e2,'\\\n",
    "        'd708e2,d794e2,d891e2,d100e3,d112e3,d126e3,d141e3,d158e3,d178e3,d200e3'\n",
    "    }\n",
    "    \n",
    "    url = 'https://avaa.tdata.fi/smear-services/smeardata.jsp?' + urlencode(query)\n",
    "    response = requests.post(url)\n",
    "\n",
    "    return pd.read_csv(io.StringIO(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Researchers plot the daily aerosol observational data fetched from SmartSMEAR in order to determine whether an event occurred and to facilitate the description of detected events. The following function implements the plotting of observational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "    d = data.copy(deep=True)\n",
    "    d = d.iloc[:, 6:].values\n",
    "    m = len(d)\n",
    "    n = len(d[0])\n",
    "    x = range(0, m)\n",
    "    y = range(0, n)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    z = np.transpose(np.array([row[1:] for row in d]).astype(np.float))\n",
    "    plt.figure(figsize=(10, 5), dpi=100)\n",
    "    plt.pcolormesh(x, y, z)\n",
    "    plt.plot((0, x.max()), (y.max()/2, y.max()/2), \"r-\")\n",
    "    plt.colorbar()\n",
    "    plt.xlim(xmax=m-1)\n",
    "    x_ticks = np.arange(x.min(), x.max(), 6)\n",
    "    x_labels = range(x_ticks.size)\n",
    "    plt.xticks(x_ticks, x_labels)\n",
    "    plt.xlabel('Hours')\n",
    "    y_ticks = np.arange(y.min(), y.max(), 6)\n",
    "    y_labels = ['3.16', '6.31', '12.6', '25.1', '50.1', '100']\n",
    "    plt.yticks(y_ticks, y_labels)\n",
    "    plt.ylabel('Diameter [nm]')\n",
    "    plt.ylim(ymax=n-1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The following code block introduces several functions needed to create, write, and visualize provenance information. \n",
    "\n",
    "The most interesting function is `create_provenance`. It takes a set of `entities` from which an entity (`entity2`) is derived. A classical simple example is a dataset that is derived from another dataset. However, a dataset may also be derived from multiple datasets. The function also takes a reference to an activity as well as the start and end times of the activity.\n",
    "\n",
    "There are three known activity types: the [data visualization activity](https://www.ebi.ac.uk/ols/ontologies/obi/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FOBI_0200111), the [averaging data transformation activity](https://www.ebi.ac.uk/ols/ontologies/obi/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FOBI_0200170), and the more general [data transformation activity](https://www.ebi.ac.uk/ols/ontologies/obi/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FOBI_0200000). These are all concepts of the [Ontology for Biomedical Investigations](http://obi-ontology.org/) (OBI). Follow the links provided here to learn more about these concepts. Don't be surprised by the term *Biomedical*. The domain here is earth and environmental sciences but these activities are ubiquitous in science. We can thus use ontologies originally developed within life sciences for our data-driven activities in earth sciences.\n",
    "\n",
    "The function `create_provenance` uses this input and creates a provenance document. Specifically, it relates the entities, activity as well as the agent (more about this below) accordingly to the [Provenance Ontology](https://www.w3.org/TR/prov-o/) (PROV-O). [Take a look](https://www.w3.org/TR/prov-o/diagrams/starting-points.svg) at these three classes and relevant relations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def create_provenance(entities, entity2, activity, starttime, endtime):\n",
    "    act = { # Defined activity types (classes)\n",
    "        'obo:OBI_0200111': 'a data visualization activity',\n",
    "        'obo:OBI_0200170': 'an averaging data transformation',\n",
    "        'obo:OBI_0200000': 'a data transformation'\n",
    "    }\n",
    "    sid = ShortId()\n",
    "    prov = ProvDocument()\n",
    "    prov.add_namespace('file', 'file:')\n",
    "    prov.add_namespace('orcid', 'https://orcid.org/')\n",
    "    prov.add_namespace('obo', 'http://purl.obolibrary.org/obo/')\n",
    "    prov.add_namespace('ex', 'http://example.org/')\n",
    "\n",
    "    e2 = prov.entity('file:{}'.format(entity2))\n",
    "    ag = prov.agent('orcid:{}'.format(orcid))\n",
    "    ac = prov.activity('ex:activity_{}'.format(sid.generate()), starttime, endtime,\n",
    "                       other_attributes={'prov:label': act[activity], \n",
    "                                         'prov:type': activity})\n",
    "    \n",
    "    prov.wasAttributedTo(e2, ag)\n",
    "    prov.wasGeneratedBy(e2, ac)\n",
    "    prov.wasAssociatedWith(ac, ag)\n",
    "        \n",
    "    for entity1 in entities:\n",
    "        e1 = prov.entity('file:{}'.format(entity1))\n",
    "        prov.wasDerivedFrom(e2, e1)\n",
    "        prov.wasAttributedTo(e1, ag)\n",
    "        prov.used(ac, e1)\n",
    "    \n",
    "    return prov\n",
    "\n",
    "def write_provenance(provenance, file):\n",
    "    provenance.serialize(file, format='rdf', rdf_format='ttl')\n",
    "        \n",
    "def visualize_provenance(provenance):\n",
    "    dot = prov_to_dot(provenance)\n",
    "    display(Image(dot.create_png()))\n",
    "    \n",
    "def new_provenance_file():\n",
    "    return 'provenance/{}.ttl'.format(datetime.now().strftime('%Y-%m-%dT%H%M%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The next code block implements the conversion of observational data into a feature vector classified by a trained artificial neural network (multilayer perceptron) to automatically assess whether an event occurred during the day. Although not central to the workflow here or the resulting provenance, such automated extraction of information about events is an interesting feature of advanced research infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "classifier_detection = joblib.load('machine-learning-model/classifier-event-detection.pkl')\n",
    "scaler_detection = joblib.load('machine-learning-model/scaler-event-detection.pkl')\n",
    "\n",
    "def feature_vector(data):\n",
    "    data_day_new = data.iloc[:, 6:20].values\n",
    "    data_daytime_new = data.loc[(data['Hour'] >= 9) & (data['Hour'] <= 15)].iloc[:, 6:20].values\n",
    "    data_nighttime_new = data.loc[(data['Hour'] < 9) | (data['Hour'] > 15)].iloc[:, 6:20].values\n",
    "    ret = [\n",
    "        np.sum(data_day_new),\n",
    "        np.max(data_day_new),\n",
    "        np.min(data_day_new),\n",
    "        np.var(data_day_new),\n",
    "        np.sum(data_nighttime_new),\n",
    "        np.max(data_nighttime_new),\n",
    "        np.min(data_nighttime_new),\n",
    "        np.var(data_nighttime_new),\n",
    "        np.sum(data_daytime_new) - np.sum(data_nighttime_new),\n",
    "        np.max(data_daytime_new) - np.max(data_nighttime_new),\n",
    "        np.min(data_daytime_new) - np.min(data_nighttime_new),\n",
    "        np.var(data_daytime_new) - np.var(data_nighttime_new)\n",
    "    ]\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Finally, we define a couple more functions that simply read and write data. Nothing suprising here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "def write_data(data, file, type='csv'):\n",
    "    if type == 'csv':\n",
    "        data.to_csv(file, index=False, encoding='utf-8')\n",
    "    elif type == 'json':\n",
    "        with open(file, 'w') as f:\n",
    "            f.write(json.dumps(data))\n",
    "    else:\n",
    "        raise ValueError('Unsupported type: {}'.format(type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Now that we have loaded modules and specialized functions as well as learned more about where the observational data are coming from and about auxiliary resources such as OBI and PROV-O, we move on to the actual workflow. Before we start, provide your [ORCID](https://orcid.org) iD. If you do not yet have an ORCID iD take a couple of minutes to [create one](https://orcid.org/register). If you do not want to create an ORCID iD you can provide your first and last name separated by a dash i.e., `FirstName-LastName`.\n",
    "\n",
    "The ORCID iD is used here to identify the agent in provenance information. You can see this by closely inspecting the `create_provenance` function introduced earlier where the `orcid` variable is used to identify the agent. All provenance information created here is attributed to this agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Provide your ORCID iD or your FirstName-LastName\n",
    "orcid = '0000-0001-5492-3212'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Data Interpretation\n",
    "\n",
    "Now we are all set up and ready to start executing our workflow. \n",
    "\n",
    "It begins with a *data interpretation* step. For a number of days, we fetch aerosol observational data from SmartSMEAR using the `fetch_data` function. The obtained data are first cached locally to a [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) file. This local caching is useful in case you want to visualize observational data for a day multiple times. Rather than fetching the data again from SmartSMEAR we can simply load them from the local cache. This pleases the SmartSMEAR admins as it reduces the number of requests. It makes also good sense here because the observational data do not change.\n",
    "\n",
    "We provide several (example) days at which an event occurred. Please process some of the provided days. Your task is to record the beginning and end times of the event by looking at the visualization of observational data. Use the following code block to configure which day you want to analyse. When you have completed a day you will need to return to this code block and select another day.\n",
    "\n",
    "Obviously, you can also select days other than the ones suggested here. You could choose a day that follows one provided here and see how the visualized observational data differs. To guide you, we also provide example days at which no event occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Days to process\n",
    "#\n",
    "# Event days\n",
    "# 2007-04-15, 2007-05-05, 2007-05-18, 2007-10-19, 2008-02-19, 2009-03-19, 2009-03-22 \n",
    "# 2011-03-15, 2011-04-19, 2011-10-01, 2012-05-01, 2012-05-29, 2013-02-20, 2013-04-04\n",
    "#\n",
    "# Non Event days\n",
    "# 2007-04-20, 2008-02-20, 2009-04-03, 2011-04-21, 2012-05-05, 2013-02-21\n",
    "\n",
    "day = '2007-04-15'\n",
    "observational_data_file = 'observational-data/{}.csv'.format(day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Next we fetch and write observational data to a file. \n",
    "\n",
    "**You only need to do this once for a day.**\n",
    "\n",
    "If you want to repeat the visualization and interpretation of a day for which you have already cached data you can skip the following code block and directly go to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "write_data(fetch_data(day), observational_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Before you continue, make sure the corresponding file was written to your `observational-data` folder. Click on the file and take a look at the observational data obtained from SmartSMEAR. It is not easy to overview these data. Hence, researchers plot them to support their analysis, specifically detecting the occurrence of new particle formation events and the description of events.\n",
    "\n",
    "Hence, we now visualize the data and based on the visualization describe the event and then store our description to a file. In addition to the day, the event description includes the beginning and end of the event i.e., the temporal duration. \n",
    "\n",
    "In addition to visualization, the following code block also uses a trained artificial neural network to automatically assess whether or not an event occurred on the day. Such automated assessment can support decision making. Naturally, automated assessments are not accurate to 100%. Hence, researchers need to review the machine assessment. To see the classifier in action, try some of the Non Event days provided above (e.g., `2007-04-20`).\n",
    "\n",
    "We also record the `starttime` and `endtime` of workflow step execution. This is provenance information, specifically of the activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "starttime = datetime.now()\n",
    "\n",
    "d = read_data(observational_data_file)\n",
    "plot_data(d)\n",
    "\n",
    "v = feature_vector(d)\n",
    "v = np.array(v).reshape(1, -1)\n",
    "v = scaler_detection.transform(v)\n",
    "\n",
    "display(Markdown('<span style=\"color:red\">Automated assessment by machine learning classifier: <span style=\"font-weight:bold\">{} Day</span></span>'.format(classifier_detection.predict(v)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Now you can interpret the visualization. Your tasks is to determine at what time the event started and ended. A new particle formation event typically begins around noon with small particles (diameter around 3 nm, which is the detection limit of the instrument). The diameter range of interest to researchers is below 25 nm. The yellow color reflects high number of particles of a given diameter size.\n",
    "\n",
    "Sometimes the event is clearly visible e.g., for `2013-04-04` but that's not always the case. Hence, there is a fairy amount of subjectivity that flows into this data interpretation process.\n",
    "\n",
    "Try to determine the time at which events begin and end by looking at the visualization. If the yellow shape does not cover the entire diameter range from 3-25 nm try to (mentally) fit a curve to estimate the times. For the end time you can use the time when particle diameter is greater than 4-6 nm, or when the yellow shape flattens out, or when it exceeds 25 nm.\n",
    "\n",
    "For the exercise here it is not relevant how accurately you estimate the beginning and end times. More important is that you get some variance in the recorded times. In the next code block you can record your estimated beginning and end time accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "event_description = {\n",
    "    'beginning': '11:00',\n",
    "    'end': '12:00'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Next we write the event description to a JSON file. In addition to the beginning and end times we also record the day. Finally, we stop recording the time used to perform the activity. Having executed the code block take a look at the JSON file in your `event-description` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "event_description['day'] = day\n",
    "\n",
    "event_description_file = 'event-description/{}.json'.format(day)\n",
    "write_data(event_description, event_description_file, type='json')\n",
    "\n",
    "endtime = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Finally, we generate provenance information for the workflow step we just executed. The executed activity is [data visualization](https://www.ebi.ac.uk/ols/ontologies/obi/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FOBI_0200111) (obo:OBI_0200111). We write the provenance information to an RDF file (in [Turtle syntax](https://www.w3.org/TR/turtle/), `ttl`) to the `provenance` directory. Make a moment to look at the generated provenance. Finally, we visualize the provenance information as a graph. This visualization is a different representation of the same information you find in the RDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "prov = create_provenance([observational_data_file], event_description_file, 'obo:OBI_0200111', starttime, endtime)\n",
    "write_provenance(prov, new_provenance_file())\n",
    "visualize_provenance(prov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Before you proceed, take a close look at the visualization of provenance above. Read the graph and identify the entities, activity and agent. Which entity was derived from which other entity in the workflow step? How long did the activity take (note that the server time is [UTC](https://en.wikipedia.org/wiki/Coordinated_Universal_Time))?\n",
    "\n",
    "Unless you have performed this workflow step for several days, you now need to go back up, select another day and perform the visual data interpretation for the selected day. Please iterate on this step for various days but make sure you move onto the next step early enough to have time to look at the next steps.\n",
    "\n",
    "If you have completed visual data interpretation for several days the next step is to compute average event duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Compute Average Event Duration\n",
    "\n",
    "We have interpreted aerosol observational data for a number of days and we now want to compute the average duration of the described events. This is the next step in the workflow in which we generate new provenance information with further entities and different activity.\n",
    "\n",
    "First, we create a dataset that includes the event descriptions as well as the duration. \n",
    "\n",
    "Take a look at the resulting dataset in the output. Are the computed durations correct?\n",
    "\n",
    "The event dataset is written as a CSV file to your `event-dataset` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "starttime = datetime.now()\n",
    "\n",
    "entities = [] # File names of input entities tracked for provenance\n",
    "event_dataset = []\n",
    "\n",
    "for file in glob.glob('event-description/*.json'):\n",
    "    entities.append(file)\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        beginning = datetime.strptime('{} {}'.format(data['day'], data['beginning']), \"%Y-%m-%d %H:%M\")\n",
    "        end = datetime.strptime('{} {}'.format(data['day'], data['end']), \"%Y-%m-%d %H:%M\")\n",
    "        data['duration'] = (end - beginning)\n",
    "        event_dataset.append(data)\n",
    "\n",
    "df = pd.DataFrame(event_dataset, columns=['day', 'beginning', 'end', 'duration'])\n",
    "\n",
    "event_dataset_file = 'event-dataset/{}.csv'.format(datetime.now().strftime('%Y-%m-%dT%H%M%S'))\n",
    "write_data(df, event_dataset_file)\n",
    "\n",
    "endtime = datetime.now()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Next we create provenance information for the performed workflow step. The executed activity is [data transformation](https://www.ebi.ac.uk/ols/ontologies/obi/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FOBI_0200000) (obo:OBI_0200000). Take a look at the generated provenance file in the `provenance` folder and a closer look at the generated visualization. Since numerous files are used as input, the visualization contains more entities. You can open the image in a separate browser tab by using the right mouse button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "prov = create_provenance(entities, event_dataset_file, 'obo:OBI_0200000', starttime, endtime)\n",
    "write_provenance(prov, new_provenance_file())\n",
    "visualize_provenance(prov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "We now compute the mean duration and create an RDF description of the duration which we store to a file in your `average-duration` folder and print to the output. As before, take a look at the created file and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "starttime = datetime.now()\n",
    "\n",
    "average = df['duration'].mean()\n",
    "\n",
    "ns = 'http://www.w3.org/2006/time#'\n",
    "\n",
    "node = BNode()\n",
    "\n",
    "graph = Graph()\n",
    "graph.bind('time', ns)\n",
    "\n",
    "graph.add((node, RDF.type, URIRef('{}Duration'.format(ns))))\n",
    "graph.add((node, URIRef('{}numericDuration'.format(ns)), Literal(average/timedelta(hours=1), datatype=XSD.decimal)))\n",
    "graph.add((node, URIRef('{}unitType'.format(ns)), URIRef('{}unitHour'.format(ns))))\n",
    "\n",
    "average_duration_file = 'average-duration/{}.ttl'.format(datetime.now().strftime('%Y-%m-%dT%H%M%S'))\n",
    "graph.serialize(destination=average_duration_file, format='ttl')\n",
    "\n",
    "endtime = datetime.now()\n",
    "\n",
    "print(graph.serialize(format='ttl').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "Finally, we create provenance for this last step of the workflow. The executed activity is [averaging data transformation](https://www.ebi.ac.uk/ols/ontologies/obi/terms?iri=http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FOBI_0200170) (obo:OBI_0200170). Take a look at the generated provenance file in the `provenance` folder and a closer look at the generated visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "prov = create_provenance([event_dataset_file], average_duration_file, 'obo:OBI_0200170', starttime, endtime)\n",
    "write_provenance(prov, new_provenance_file())\n",
    "visualize_provenance(prov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## Inspect Provenance\n",
    "\n",
    "In this third part, we inspect provenance information using SPARQL queries. First, we read all RDF files with provenance information found in your `provenance` into a graph. Furthermore, we introduce an additional function which executes a SPARQL query and displays query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "\n",
    "for file in glob.glob('provenance/*.ttl'):\n",
    "    r = g.parse(file, format='turtle')\n",
    "\n",
    "def query(q):\n",
    "    serializer = CSVResultSerializer(g.query(q))\n",
    "    output = io.BytesIO()\n",
    "    serializer.serialize(output)\n",
    "    display(pd.read_csv(io.StringIO(output.getvalue().decode())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The following query is for all entities that are derived from entities, and the corresponding activity. You can follow which event descriptions are derived from which observational data in data visualization activities; which event datasets are derived from which event descriptions in data transformation activities; and which average durations are derived from which event datasets in averaging data transformation activities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "query(\"\"\"\n",
    "PREFIX prov: <http://www.w3.org/ns/prov#> \n",
    "SELECT ?entity1 ?entity2 ?activity WHERE { \n",
    "  ?entity2 prov:wasDerivedFrom ?entity1 .\n",
    "  ?entity2 prov:wasGeneratedBy [\n",
    "      rdfs:label ?activity\n",
    "  ]\n",
    "}\n",
    "ORDER BY (?entity1)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "The following query computes the average duration of activities, in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "query(\"\"\"\n",
    "PREFIX prov: <http://www.w3.org/ns/prov#> \n",
    "SELECT (avg(?duration) as ?duration) WHERE { \n",
    "  ?activity prov:startedAtTime ?startTime .\n",
    "  ?activity prov:endedAtTime ?endTime .\n",
    "#  ?activity rdfs:label \"a data visualization activity\"^^xsd:string\n",
    "  BIND (\n",
    "      (\n",
    "          (3600*hours(?endTime)+60*minutes(?endTime)+seconds(?endTime)) - \n",
    "          (3600*hours(?startTime)+60*minutes(?startTime)+seconds(?startTime)\n",
    "        )\n",
    "    ) AS ?duration)\n",
    "}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
